{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10645567,
          "sourceType": "datasetVersion",
          "datasetId": 6591694
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gredy/2021Z-DataVisualizationTechniques/blob/master/Copy_of_Mini_Kaggle_Project_1_Breast_Cancer_Classification_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:21.975088Z",
          "iopub.execute_input": "2025-02-03T04:38:21.975373Z",
          "iopub.status.idle": "2025-02-03T04:38:21.983177Z",
          "shell.execute_reply.started": "2025-02-03T04:38:21.975356Z",
          "shell.execute_reply": "2025-02-03T04:38:21.982461Z"
        },
        "id": "BSl7E55sSCyd",
        "outputId": "d3890dac-41bb-4dfb-cc46-6ed332f4e01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/mini-kaggle-1-data-set/train.csv\n/kaggle/input/mini-kaggle-1-data-set/test.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import  numpy as np\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv('sample_data/train.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:21.987499Z",
          "iopub.execute_input": "2025-02-03T04:38:21.987691Z",
          "iopub.status.idle": "2025-02-03T04:38:22.019046Z",
          "shell.execute_reply.started": "2025-02-03T04:38:21.987673Z",
          "shell.execute_reply": "2025-02-03T04:38:22.017912Z"
        },
        "id": "-zzFpkGhSCye"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.020271Z",
          "iopub.execute_input": "2025-02-03T04:38:22.020625Z",
          "iopub.status.idle": "2025-02-03T04:38:22.043330Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.020601Z",
          "shell.execute_reply": "2025-02-03T04:38:22.042478Z"
        },
        "id": "jzOq5zZASCyf",
        "outputId": "9cea9e5c-837a-4fd6-f9c8-a9962de6cdb7"
      },
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         id label  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0  90524101     M        17.99         20.66          117.80      991.7   \n1  84358402     M        20.29         14.34          135.10     1297.0   \n2     89346     B         9.00         14.40           56.36      246.3   \n3    902975     B        12.21         14.09           78.78      462.0   \n4    904969     B        12.34         14.95           78.29      469.1   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.10360           0.13040        0.120100             0.088240   \n1          0.10030           0.13280        0.198000             0.104300   \n2          0.07005           0.03116        0.003681             0.003472   \n3          0.08108           0.07823        0.068390             0.025340   \n4          0.08682           0.04571        0.021090             0.020540   \n\n   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0  ...        21.080          25.41           138.10      1349.0   \n1  ...        22.540          16.67           152.20      1575.0   \n2  ...         9.699          20.07            60.90       285.5   \n3  ...        13.130          19.29            87.65       529.9   \n4  ...        13.180          16.85            84.11       533.1   \n\n   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n0           0.14820            0.37350          0.33010               0.19740   \n1           0.13740            0.20500          0.40000               0.16250   \n2           0.09861            0.05232          0.01472               0.01389   \n3           0.10260            0.24310          0.30760               0.09140   \n4           0.10480            0.06744          0.04921               0.04793   \n\n   symmetry_worst  fractal_dimension_worst  \n0          0.3060                  0.08503  \n1          0.2364                  0.07678  \n2          0.2991                  0.07804  \n3          0.2677                  0.08824  \n4          0.2298                  0.05974  \n\n[5 rows x 32 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90524101</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>20.66</td>\n      <td>117.80</td>\n      <td>991.7</td>\n      <td>0.10360</td>\n      <td>0.13040</td>\n      <td>0.120100</td>\n      <td>0.088240</td>\n      <td>...</td>\n      <td>21.080</td>\n      <td>25.41</td>\n      <td>138.10</td>\n      <td>1349.0</td>\n      <td>0.14820</td>\n      <td>0.37350</td>\n      <td>0.33010</td>\n      <td>0.19740</td>\n      <td>0.3060</td>\n      <td>0.08503</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.198000</td>\n      <td>0.104300</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.40000</td>\n      <td>0.16250</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>89346</td>\n      <td>B</td>\n      <td>9.00</td>\n      <td>14.40</td>\n      <td>56.36</td>\n      <td>246.3</td>\n      <td>0.07005</td>\n      <td>0.03116</td>\n      <td>0.003681</td>\n      <td>0.003472</td>\n      <td>...</td>\n      <td>9.699</td>\n      <td>20.07</td>\n      <td>60.90</td>\n      <td>285.5</td>\n      <td>0.09861</td>\n      <td>0.05232</td>\n      <td>0.01472</td>\n      <td>0.01389</td>\n      <td>0.2991</td>\n      <td>0.07804</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>902975</td>\n      <td>B</td>\n      <td>12.21</td>\n      <td>14.09</td>\n      <td>78.78</td>\n      <td>462.0</td>\n      <td>0.08108</td>\n      <td>0.07823</td>\n      <td>0.068390</td>\n      <td>0.025340</td>\n      <td>...</td>\n      <td>13.130</td>\n      <td>19.29</td>\n      <td>87.65</td>\n      <td>529.9</td>\n      <td>0.10260</td>\n      <td>0.24310</td>\n      <td>0.30760</td>\n      <td>0.09140</td>\n      <td>0.2677</td>\n      <td>0.08824</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>904969</td>\n      <td>B</td>\n      <td>12.34</td>\n      <td>14.95</td>\n      <td>78.29</td>\n      <td>469.1</td>\n      <td>0.08682</td>\n      <td>0.04571</td>\n      <td>0.021090</td>\n      <td>0.020540</td>\n      <td>...</td>\n      <td>13.180</td>\n      <td>16.85</td>\n      <td>84.11</td>\n      <td>533.1</td>\n      <td>0.10480</td>\n      <td>0.06744</td>\n      <td>0.04921</td>\n      <td>0.04793</td>\n      <td>0.2298</td>\n      <td>0.05974</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING"
      ],
      "metadata": {
        "id": "vhrCwb24SCyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load the original training dataset (which contains the test set)\n",
        "df_original = pd.read_csv(\"sample_data/train.csv\")\n",
        "\n",
        "# Separate features and target variable for the entire dataset\n",
        "df = df_original.drop(columns=[\"id\"])  # Drop the 'id' column for feature scaling\n",
        "X = df.drop(columns=[\"label\"])\n",
        "y = df[\"label\"]  # Keep 'label' as it is ('M'/'B')\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the features for training data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get the test data\n",
        "X_test_original = df_original[-len(X_test):]  # Get the original test data\n",
        "test_ids = X_test_original['id']  # Extract the 'id' column for the original test set\n",
        "\n",
        "# Scale the test data (excluding 'id' and 'label' columns)\n",
        "X_test_scaled = scaler.transform(X_test_original.drop(columns=[\"id\", \"label\"]))\n",
        "\n",
        "# Load dataset for test.csv\n",
        "test_data = pd.read_csv(\"sample_data/test.csv\")\n",
        "\n",
        "# Process the test dataset\n",
        "X_test_final = test_data.drop(columns=[\"id\"])  # Drop the 'id' column\n",
        "X_test_final_scaled = scaler.transform(X_test_final)  # Apply the same scaling"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.044691Z",
          "iopub.execute_input": "2025-02-03T04:38:22.044869Z",
          "iopub.status.idle": "2025-02-03T04:38:22.071209Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.044852Z",
          "shell.execute_reply": "2025-02-03T04:38:22.070411Z"
        },
        "id": "DtI1qk7YSCyf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PERCEPTRON MODEL\n",
        "\n",
        "* A basic neural network model that learns through updates based on misclassified points.\n",
        "* Perceptron is sensitive to feature scaling, so we use StandardScaler()."
      ],
      "metadata": {
        "id": "qhzZeD8SSCyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Initialize and train the Perceptron model\n",
        "perceptron = Perceptron(random_state=42)\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for the test set\n",
        "y_pred = perceptron.predict(X_test_scaled)\n",
        "\n",
        "# Get predictions for the test data\n",
        "y_pred_final = perceptron.predict(X_test_final_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.072503Z",
          "iopub.execute_input": "2025-02-03T04:38:22.072674Z",
          "iopub.status.idle": "2025-02-03T04:38:22.078641Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.072658Z",
          "shell.execute_reply": "2025-02-03T04:38:22.077858Z"
        },
        "id": "DxPcOWhaSCyg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION\n",
        "\n",
        "* LogisticRegression(random_state=42, max_iter=1000) ensures convergence for complex datasets.\n",
        "* Prevents issues caused by varying feature scales."
      ],
      "metadata": {
        "id": "-hvYZe76SCyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Get predictions for test.csv\n",
        "y_pred_final = log_reg.predict(X_test_final_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.079311Z",
          "iopub.execute_input": "2025-02-03T04:38:22.079547Z",
          "iopub.status.idle": "2025-02-03T04:38:22.100701Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.079530Z",
          "shell.execute_reply": "2025-02-03T04:38:22.100011Z"
        },
        "id": "h-nMpw_aSCyg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "\n",
        "* SVC(kernel='rbf') works well for complex decision boundaries.\n",
        "* SVM is sensitive to feature scales, so scaling is necessary."
      ],
      "metadata": {
        "id": "jF5rIgjuSCyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model = SVC(kernel='rbf', random_state=42)  # Using RBF kernel\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Get predictions for test.csv\n",
        "y_pred_final = svm_model.predict(X_test_final_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.101600Z",
          "iopub.execute_input": "2025-02-03T04:38:22.101789Z",
          "iopub.status.idle": "2025-02-03T04:38:22.116549Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.101772Z",
          "shell.execute_reply": "2025-02-03T04:38:22.115809Z"
        },
        "id": "PRlIb9PZSCyh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECISION TREES\n",
        "\n",
        "* DecisionTreeClassifier(max_depth=5) prevents overfitting.\n",
        "* Helps models perform better, especially with distance-based decisions."
      ],
      "metadata": {
        "id": "eiN_TUTYSCyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42, max_depth=5)  # Using max_depth=5 to prevent overfitting\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Get predictions for test.csv\n",
        "y_pred_final = dt_model.predict(X_test_final_scaled)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.117243Z",
          "iopub.execute_input": "2025-02-03T04:38:22.117475Z",
          "iopub.status.idle": "2025-02-03T04:38:22.127160Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.117454Z",
          "shell.execute_reply": "2025-02-03T04:38:22.126458Z"
        },
        "id": "Mp2kLwwCSCyh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN\n",
        "\n",
        "* KNeighborsClassifier(n_neighbors=5) considers 5 nearest points for classification.\n",
        "* KNN is sensitive to different feature scales, so StandardScaler is necessary."
      ],
      "metadata": {
        "id": "kuTDlroiSCyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Train the KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Get predictions for test.csv\n",
        "y_pred_final = knn_model.predict(X_test_final_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.128915Z",
          "iopub.execute_input": "2025-02-03T04:38:22.129129Z",
          "iopub.status.idle": "2025-02-03T04:38:22.147043Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.129108Z",
          "shell.execute_reply": "2025-02-03T04:38:22.146308Z"
        },
        "id": "brby5dtnSCyh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST\n",
        "\n",
        "* RandomForestClassifier(n_estimators=100) creates 100 decision trees for better accuracy.\n",
        "* While Random Forest isn't sensitive to scaling, keeping it consistent with previous models is good practice."
      ],
      "metadata": {
        "id": "IafDYDnkSCyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Get predictions for test.csv\n",
        "y_pred_final = rf_model.predict(X_test_final_scaled)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.147807Z",
          "iopub.execute_input": "2025-02-03T04:38:22.148039Z",
          "iopub.status.idle": "2025-02-03T04:38:22.299285Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.148016Z",
          "shell.execute_reply": "2025-02-03T04:38:22.298430Z"
        },
        "id": "Tf9N50LeSCyi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Submission"
      ],
      "metadata": {
        "id": "KUWjLXsISCyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Evaluate performance on validation set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print validation results\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset for test.csv\n",
        "test_data = pd.read_csv(\"sample_data/test.csv\")\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],  # Retain the 'id' column from test.csv\n",
        "    'label': y_pred_final  # Include the predicted labels\n",
        "})\n",
        "\n",
        "# Save to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Final submission file created with {len(submission)} rows.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T04:38:22.300060Z",
          "iopub.execute_input": "2025-02-03T04:38:22.300230Z",
          "iopub.status.idle": "2025-02-03T04:38:22.317383Z",
          "shell.execute_reply.started": "2025-02-03T04:38:22.300215Z",
          "shell.execute_reply": "2025-02-03T04:38:22.316412Z"
        },
        "id": "PVzdw4xuSCyi",
        "outputId": "f4066b9b-8a32-4709-8873-f35115ebdbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9670\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.97      0.98        63\n",
            "           M       0.93      0.96      0.95        28\n",
            "\n",
            "    accuracy                           0.97        91\n",
            "   macro avg       0.96      0.97      0.96        91\n",
            "weighted avg       0.97      0.97      0.97        91\n",
            "\n",
            "Final submission file created with 114 rows.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision\n",
        "\n",
        "Upon evaluation Perceptron and Logistic Regression both equally scored the best at 0.98245. I decided to choose Logistic Regression as my final submission. In most cases, Logistic Regression is the better model because it is more robust, reliable, and provides probability outputs."
      ],
      "metadata": {
        "id": "jtSEXPweSCyi"
      }
    }
  ]
}